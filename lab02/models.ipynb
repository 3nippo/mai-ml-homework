{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel:\n",
    "    def check_value_and_set(self, name, value, allowed):\n",
    "        message = \"Wrong {}: '{}'\\n\\nAllowed values: {}\"\n",
    "        \n",
    "        if value not in allowed:\n",
    "            raise ValueError(message.format(\n",
    "                name,\n",
    "                value,\n",
    "                allowed\n",
    "            ))\n",
    "        \n",
    "        setattr(self, name, value)\n",
    "        \n",
    "    def check_value_type_and_set(self, name, value, allowed):\n",
    "        message = \"Wrong {} type: '{}'\\n\\nAllowed types: {}\"\n",
    "        \n",
    "        if not isinstance(value, allowed):\n",
    "            raise TypeError(message.format(\n",
    "                name,\n",
    "                value,\n",
    "                allowed\n",
    "            ))\n",
    "        \n",
    "        setattr(self, name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\n",
    "#     '../lab01/nyc-taxi-trip-duration/cleaned_train.csv'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(BasicModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        penalty='l2',\n",
    "        tol=1e-4,\n",
    "        C=1.0,\n",
    "        fit_intercept=True,\n",
    "        max_iter=100\n",
    "    ):\n",
    "        super().check_value_and_set(\n",
    "            'penalty',\n",
    "            penalty,\n",
    "            ['l1', 'l2', None]\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'tol',\n",
    "            tol,\n",
    "            (int, float)\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'C',\n",
    "            C,\n",
    "            (int, float)\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'fit_intercept',\n",
    "            fit_intercept,\n",
    "            bool\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'max_iter',\n",
    "            max_iter,\n",
    "            int\n",
    "        )\n",
    "    \n",
    "    def __get_l1_penalty(self):\n",
    "        def l1_penalty(w):\n",
    "            return 1/self.C * np.abs(w)\n",
    "        \n",
    "        def der_l1_penalty(w):\n",
    "            # ignoring zeros existence\n",
    "            return 1/self.C * ((w > 0) * 1 + (w <= 0) * -1)\n",
    "        \n",
    "        return l1_penalty, der_l1_penalty\n",
    "    \n",
    "    def __get_l2_penalty(self):\n",
    "        def l2_penalty(w):\n",
    "            return 1/self.C * np.multiply(w, w)\n",
    "        \n",
    "        def der_l2_penalty(w):\n",
    "            return 2/self.C * w\n",
    "        \n",
    "        return l2_penalty, der_l2_penalty\n",
    "    \n",
    "    def __get_None_penalty(self):\n",
    "        return None, None\n",
    "    \n",
    "    def fit(self, X, y, debug=False):\n",
    "        assert \\\n",
    "            len(X.shape) == 2, \\\n",
    "            \"X should be 2D vector\"\n",
    "        assert \\\n",
    "            y.shape == (X.shape[0], 1), \\\n",
    "            \"y should be 2D vector and should correspond to X\"\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((\n",
    "                X, \n",
    "                np.ones(\n",
    "                    (X.shape[0], 1)\n",
    "                )\n",
    "            ))\n",
    "        \n",
    "        args = [X, y]\n",
    "        \n",
    "        args.extend(\n",
    "            getattr(\n",
    "                self,\n",
    "                '_LogisticRegression__get_' + str(self.penalty) + '_penalty'\n",
    "            )()\n",
    "        )\n",
    "        \n",
    "        self.w = np.ones((X.shape[1], 1))\n",
    "        \n",
    "        if debug:\n",
    "            return args\n",
    "        \n",
    "        result = sp.optimize.minimize(\n",
    "            self.__cost,\n",
    "            self.w,\n",
    "            args,\n",
    "            'L-BFGS-B',\n",
    "            self.__gradient,\n",
    "            tol=self.tol,\n",
    "            options={\n",
    "                'maxiter': self.max_iter\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        assert result.success, result.message\n",
    "        \n",
    "        self.w = result.x\n",
    "    \n",
    "    @staticmethod\n",
    "    def __predict(X, w):\n",
    "        def predict_real(x, w):\n",
    "            return x @ w\n",
    "\n",
    "        def sigmoid(z):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        \n",
    "        return sigmoid(predict_real(X, w))\n",
    "    \n",
    "    @staticmethod\n",
    "    def __cost(w, args):\n",
    "        X, y, penalty, _ = args\n",
    "        \n",
    "        predictions = LogisticRegression.__predict(X, w)\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        \n",
    "        cost0 = -(1 - y).T @ np.log(1 - predictions)\n",
    "        cost1 = -y.T @ np.log(predictions)\n",
    "        \n",
    "        penalty_part = penalty(w).sum() if penalty else 0\n",
    "        \n",
    "        final_cost = (cost0 + cost1).sum() / m + penalty_part\n",
    "        \n",
    "        return final_cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((\n",
    "                X, \n",
    "                np.ones(\n",
    "                    (X.shape[0], 1)\n",
    "                )\n",
    "            ))\n",
    "        return self.__predict(X, self.w)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __gradient(w, args):\n",
    "        X, y, _, der_penalty = args\n",
    "        w = w.reshape((-1, 1))\n",
    "        \n",
    "        predictions = LogisticRegression.__predict(X, w)\n",
    "        \n",
    "        penalty_part = der_penalty(w) if der_penalty else 0\n",
    "        \n",
    "        return X.T @ (predictions - y) + penalty_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import unittest\n",
    "\n",
    "def dummy_dataset():\n",
    "    X, y = make_classification(100, 20)\n",
    "    y = y.reshape((100, 1))\n",
    "    return X, y\n",
    "\n",
    "def prepare(debug=True, penalty=None):\n",
    "    X, y = dummy_dataset()\n",
    "\n",
    "    lr = LogisticRegression(penalty=penalty)\n",
    "\n",
    "    args = lr.fit(X, y, debug)\n",
    "    \n",
    "    return lr, X, y, args\n",
    "\n",
    "class TestLogisticRegression(unittest.TestCase):\n",
    "    def test_gradient(self):\n",
    "        lr, X, y, args = prepare()\n",
    "        \n",
    "        self.assertEqual(\n",
    "            lr._LogisticRegression__gradient(lr.w, args).shape,\n",
    "            (21, 1)\n",
    "        )\n",
    "        \n",
    "    def test_cost(self):\n",
    "        lr, X, y, args = prepare()\n",
    "        \n",
    "        self.assertEqual(\n",
    "            type(lr._LogisticRegression__cost(lr.w, args)),\n",
    "            np.float64\n",
    "        )\n",
    "    \n",
    "    def test_None(self):\n",
    "        lr, X, y, args = prepare(False)\n",
    "        score = roc_auc_score(y, lr.predict(X))\n",
    "        print(\"Score: {}\".format(score))\n",
    "        \n",
    "    def test_l1(self):\n",
    "        lr, X, y, args = prepare(False, 'l1')\n",
    "        score = roc_auc_score(y, lr.predict(X))\n",
    "        print(\"Score: {}\".format(score))\n",
    "    \n",
    "    def test_l2(self):\n",
    "        lr, X, y, args = prepare(False, 'l2')\n",
    "        score = roc_auc_score(y, lr.predict(X))\n",
    "        print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_None (__main__.TestLogisticRegression) ... /home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:126: RuntimeWarning: divide by zero encountered in log\n",
      "/home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:126: RuntimeWarning: invalid value encountered in matmul\n",
      "/home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:114: RuntimeWarning: overflow encountered in exp\n",
      "/home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:127: RuntimeWarning: divide by zero encountered in log\n",
      "/home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:127: RuntimeWarning: invalid value encountered in matmul\n",
      "ok\n",
      "test_cost (__main__.TestLogisticRegression) ... ok\n",
      "test_gradient (__main__.TestLogisticRegression) ... ok\n",
      "test_l1 (__main__.TestLogisticRegression) ... ok\n",
      "test_l2 (__main__.TestLogisticRegression) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n",
      "Score: 0.9863945578231293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAIL\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_l2 (__main__.TestLogisticRegression)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-19-30bff43b0f2f>\", line 47, in test_l2\n",
      "    lr, X, y, args = prepare(False, 'l2')\n",
      "  File \"<ipython-input-19-30bff43b0f2f>\", line 15, in prepare\n",
      "    args = lr.fit(X, y, debug)\n",
      "  File \"<ipython-input-18-12246dbbe0cf>\", line 104, in fit\n",
      "    assert result.success, result.message\n",
      "AssertionError: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.038s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f505ad0f5d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['first-arg-is-ignored', '--verbose'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes this happens due to too large l2 normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_None (__main__.TestLogisticRegression) ... /home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:126: RuntimeWarning: divide by zero encountered in log\n",
      "/home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:126: RuntimeWarning: invalid value encountered in matmul\n",
      "/home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:114: RuntimeWarning: overflow encountered in exp\n",
      "/home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:127: RuntimeWarning: divide by zero encountered in log\n",
      "/home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:127: RuntimeWarning: invalid value encountered in matmul\n",
      "ok\n",
      "test_cost (__main__.TestLogisticRegression) ... ok\n",
      "test_gradient (__main__.TestLogisticRegression) ... ok\n",
      "test_l1 (__main__.TestLogisticRegression) ... ok\n",
      "test_l2 (__main__.TestLogisticRegression) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n",
      "Score: 0.9867947178871549\n",
      "Score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.042s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f505acaf910>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['first-arg-is-ignored', '--verbose'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import functools\n",
    "\n",
    "# CART DT (maybe)\n",
    "class DecisionTree(BasicModel):\n",
    "    class Node:\n",
    "        def __init__(self, feature, value, observation_indexes):\n",
    "            self.feature = feature\n",
    "            self.split_value = value\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.answer = None\n",
    "            self.observation_indexes = observation_indexes\n",
    "            \n",
    "        def is_leaf(self):\n",
    "            return self.left or self.right\n",
    "    \n",
    "    criterion_name_to_calculator = {\n",
    "        'gini': '_DecisionTree__gini_index'\n",
    "    }\n",
    "    \n",
    "    criterion_name_to_cmp = {\n",
    "        'gini': lambda x, y: x-y\n",
    "    }\n",
    "    \n",
    "    criterion_name_to_task = {\n",
    "        'gini': 'classification',\n",
    "        'entropy': 'classification',\n",
    "        'gain_info': 'classification',\n",
    "        'mse': 'regression',\n",
    "        'mae': 'regression'\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        criterion='gini',\n",
    "        splitter='best',\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        max_features=None,\n",
    "        random_state=42,\n",
    "        allow_pruning=False  # node saves corresponding to it observation indexes\n",
    "    ):\n",
    "        super().check_value_and_set(\n",
    "            'criterion',\n",
    "            criterion,\n",
    "            ['gini', 'entropy', 'gain_info', 'mse', 'mae']\n",
    "        )\n",
    "        \n",
    "        self.__cmp_criterion_values = \\\n",
    "            self.criterion_name_to_cmp[criterion]\n",
    "        self.__calc_criterion_value = \\\n",
    "            self.criterion_name_to_calculator[criterion]\n",
    "        \n",
    "        self.__task = criterion_name_to_task[criterion]\n",
    "        \n",
    "        super().check_value_and_set(\n",
    "            'splitter',\n",
    "            splitter,\n",
    "            ['best', 'random']\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'max_depth',\n",
    "            max_depth,\n",
    "            (int, type(None))\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'min_samples_split',\n",
    "            min_samples_split,\n",
    "            (int, float)\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'min_weight_fraction_leaf',\n",
    "            min_weight_fraction_leaf,\n",
    "            float\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'max_features',\n",
    "            max_features,\n",
    "            (int, float, str, type(None))\n",
    "        )\n",
    "        \n",
    "        if type(max_features) == str:\n",
    "            super().check_value_and_set(\n",
    "                'max_features',\n",
    "                max_features,\n",
    "                ['auto', 'sqrt', 'log2']\n",
    "            )\n",
    "        if max_features == 'auto':\n",
    "            max_features = 'sqrt'\n",
    "            \n",
    "        super().check_value_type_and_set(\n",
    "            'random_state',\n",
    "            random_state,\n",
    "            (np.random.RandomState, int)\n",
    "        )\n",
    "        if type(random_state) == int:\n",
    "            self.random_state = np.random.RandomState(random_state)\n",
    "            \n",
    "        super().check_value_type_and_set(\n",
    "            'allow_pruning',\n",
    "            allow_pruning,\n",
    "            bool\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        assert \\\n",
    "            len(X.shape) == 2, \\\n",
    "            \"X should be 2D vector\"\n",
    "        assert \\\n",
    "            y.shape == (X.shape[0], 1), \\\n",
    "            \"y should be 2D vector and should correspond to X\"\n",
    "        \n",
    "        self.X, self.y = X.to_numpy(), y.to_numpy()\n",
    "        \n",
    "        if max_features != None:\n",
    "            if type(max_features) == float:\n",
    "                max_features = np.floor(X.shape[1] * max_features)\n",
    "            elif type(max_features) == str:\n",
    "                max_features = np.floor(\n",
    "                    getattr(np, max_features)(X.shape[1])\n",
    "                )\n",
    "        \n",
    "        self.__construct_tree()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        \n",
    "        for el in X:\n",
    "            prediction = self.__predict_observation(el)\n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        return np.array(predictions).reshape((-1, 1))\n",
    "            \n",
    "    def __predict_observation(self, el):\n",
    "        node = self.root\n",
    "        \n",
    "        while not node.is_leaf():\n",
    "            prev_node = node\n",
    "            \n",
    "            if el[node.feature] <= node.split_value:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        \n",
    "        return node.answer\n",
    "    \n",
    "    def __gini_impurity(self, observation_indexes):        \n",
    "        node_y = self.y[[observation_indexes], :]\n",
    "        \n",
    "        p_0 = (node_y == 0).sum()\n",
    "        p_1 = (node_y == 1).sum()\n",
    "        \n",
    "        return 1 - p_0 * p_0 - p_1 * p_1\n",
    "    \n",
    "    def __gini_index(\n",
    "        self, \n",
    "        left_split_indexes, \n",
    "        right_split_indexes\n",
    "    ):\n",
    "        w_left = len(left_split_indexes)\n",
    "        w_right = len(right_split_indexes)\n",
    "        \n",
    "        W = w_left + w_right\n",
    "        \n",
    "        gini_index = w_left/W * self.__gini_impurity(left_split_indexes)\n",
    "        \n",
    "        gini_index += w_right/W * self.__gini_impurity(right_split_indexes)\n",
    "        \n",
    "        return gini_index\n",
    "    \n",
    "    def __get_split_pairs_gen(\n",
    "        self,\n",
    "        observation_indexes\n",
    "    ):\n",
    "        node_x = self.X[[observation_indexes], [feature]]\n",
    "        \n",
    "        observation_indexes, node_x = zip(\n",
    "            *sorted(\n",
    "                zip(\n",
    "                    observation_indexes,\n",
    "                    node_x.ravel()\n",
    "                ), \n",
    "                key=lambda x: x[1]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # to use numpy views\n",
    "        observation_indexes = np.array(observation_indexes)\n",
    "        \n",
    "        uniques = list(dict.fromkeys(node_x).keys())\n",
    "        \n",
    "        last_i = 0\n",
    "        id_unique = 0\n",
    "        \n",
    "        while id_unique < len(uniques) - 1:\n",
    "            for i in range(last_i, len(node_x)):\n",
    "                if node_x[i] > uniques[id_unique]:\n",
    "                    last_i = i\n",
    "                    break\n",
    "            \n",
    "            left_split_indexes = observation_indexes[:last_i]\n",
    "            right_split_indexes = observation_indexes[last_i:]\n",
    "            \n",
    "            yield left_split_indexes, right_split_indexes, uniques[id_unique]\n",
    "            \n",
    "            id_unique += 1\n",
    "    \n",
    "    BestFeatureSplit = namedtuple(\n",
    "        'BestFeatureSplit', \n",
    "        [\n",
    "            'criterion_value',\n",
    "            'split_value',\n",
    "            'left_split_indexes',\n",
    "            'right_split_indexes'\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # output can be (None, None)\n",
    "    def __find_best_split_by_feature(\n",
    "        self, \n",
    "        feature, \n",
    "        observation_indexes\n",
    "    ):\n",
    "        split_pairs_gen = self.__get_split_pairs_gen(\n",
    "            observation_indexes\n",
    "        )\n",
    "        \n",
    "        # find best split\n",
    "        best_split = DecisionTree.BestFeatureSplit(\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None\n",
    "        )\n",
    "        \n",
    "        for left_split_indexes, right_split_indexes, split_value in split_pairs_gen:\n",
    "            criterion_value = self.__calc_criterion_value(\n",
    "                left_split_indexes,\n",
    "                right_split_indexes\n",
    "            )\n",
    "            \n",
    "            current_split = DecisionTree.BestFeatureSplit(\n",
    "                criterion_value,\n",
    "                split_value,\n",
    "                left_split_indexes,\n",
    "                right_split_indexes\n",
    "            )\n",
    "            \n",
    "            if best_criterion_value is None or \\\n",
    "               self.__cmp_criterion_values(\n",
    "                   criterion_value, \n",
    "                   best_criterion_value\n",
    "               ) < 0:\n",
    "                best_split = current_split\n",
    "        \n",
    "        return best_split\n",
    "\n",
    "    def __construct_tree_helper(self, observation_indexes, depth):\n",
    "        if depth > self.max_depth or \\\n",
    "           observation_indexes.shape[0] < self.min_samples_split:\n",
    "            return None\n",
    "        \n",
    "        features = np.arange(self.X.shape[1])\n",
    "        if self.splitter == 'random':\n",
    "            self.random_state.shuffle(features)\n",
    "        \n",
    "        best_feature_split = None\n",
    "        best_feature = None\n",
    "    \n",
    "        feature_counter = 0 if self.max_features else float('inf')\n",
    "        \n",
    "        cmp_criterion_values = self.__get_criterion_cmp()\n",
    "        \n",
    "        for feature in features:\n",
    "            if best_feature and feature_counter > self.max_features:\n",
    "                break\n",
    "            \n",
    "            feature_split = self.__find_best_split_by_feature(\n",
    "                feature,\n",
    "                observation_indexes\n",
    "            )\n",
    "            \n",
    "            if feature_split.criterion_value is None:\n",
    "                continue\n",
    "            \n",
    "            if best_feature is None or \\\n",
    "               cmp_criterion_values(\n",
    "                   feature_split.criterion_value,\n",
    "                   best_feature_split.criterion_value\n",
    "               ) < 0:\n",
    "                best_feature_split = feature_split\n",
    "                best_feature = feature\n",
    "            \n",
    "            feature_counter += 1\n",
    "        \n",
    "        if best_feature is None or \\\n",
    "           best_feature_split.criterion_value == 0:\n",
    "            return None\n",
    "        \n",
    "        node = DecisionTree.Node(\n",
    "            best_feature,\n",
    "            best_feature_split.split_value,\n",
    "            observation_indexes\n",
    "        )\n",
    "        \n",
    "        node.left = self.__construct_tree_helper(\n",
    "            best_feature_split.left_split_indexes,\n",
    "            depth + 1\n",
    "        )\n",
    "        \n",
    "        node.right = self.__construct_tree_helper(\n",
    "            best_feature_split.right_split_indexes,\n",
    "            depth + 1\n",
    "        )\n",
    "        \n",
    "        if node.is_leaf():\n",
    "            node.answer = self.__get_answer(\n",
    "                observation_indexes\n",
    "            )\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def __get_answer(self, observation_indexes):\n",
    "        if self.__task == 'classification':\n",
    "            ones = self.y[[observation_indexes]].sum()\n",
    "            threshold = y.shape[0] / 2\n",
    "            \n",
    "            if ones > threshold:\n",
    "                return 1\n",
    "            elif ones < threshold:\n",
    "                return 0\n",
    "            else:\n",
    "                return self.random_state.randint(0, 1)\n",
    "        else:\n",
    "            return np.mean(y)\n",
    "    \n",
    "    def __construct_tree(self):\n",
    "        observation_indexes = np.arange(self.y.shape[0])\n",
    "        \n",
    "        self.root = self.__construct_tree_helper(\n",
    "            observation_indexes, \n",
    "            1\n",
    "        )\n",
    "        \n",
    "        if self.root is None:\n",
    "            self.root = DecisionTree.Node(\n",
    "                None,\n",
    "                None,\n",
    "                observation_indexes\n",
    "            )\n",
    "            \n",
    "            self.answer = self.__get_answer(\n",
    "                observation_indexes\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DecisionTree.__gini_index of <__main__.DecisionTree object at 0x7f505ad43c10>>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import unittest\n",
    "\n",
    "def dummy_dataset():\n",
    "    X, y = make_classification(100, 20)\n",
    "    y = y.reshape((100, 1))\n",
    "    return X, y\n",
    "\n",
    "def fit_and_score(*args, **kwargs):\n",
    "    X, y = dummy_dataset()\n",
    "\n",
    "    dt = DecisionTree(*args, **kwargs)\n",
    "\n",
    "    dt.fit(X, y)\n",
    "    \n",
    "    return dt, X, y, args\n",
    "\n",
    "class TestLogisticRegression(unittest.TestCase):\n",
    "    def test_gini(self):\n",
    "        lr, X, y, args = prepare()\n",
    "        \n",
    "        self.assertEqual(\n",
    "            lr._LogisticRegression__gradient(lr.w, args).shape,\n",
    "            (21, 1)\n",
    "        )\n",
    "        \n",
    "    def test_cost(self):\n",
    "        lr, X, y, args = prepare()\n",
    "        \n",
    "        self.assertEqual(\n",
    "            type(lr._LogisticRegression__cost(lr.w, args)),\n",
    "            np.float64\n",
    "        )\n",
    "    \n",
    "    def test_None(self):\n",
    "        lr, X, y, args = prepare(False)\n",
    "        score = roc_auc_score(y, lr.predict(X))\n",
    "        print(\"Score: {}\".format(score))\n",
    "        \n",
    "    def test_l1(self):\n",
    "        lr, X, y, args = prepare(False, 'l1')\n",
    "        score = roc_auc_score(y, lr.predict(X))\n",
    "        print(\"Score: {}\".format(score))\n",
    "    \n",
    "    def test_l2(self):\n",
    "        lr, X, y, args = prepare(False, 'l2')\n",
    "        score = roc_auc_score(y, lr.predict(X))\n",
    "        print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
