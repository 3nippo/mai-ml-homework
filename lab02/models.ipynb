{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel:\n",
    "    def check_value_and_set(self, name, value, allowed):\n",
    "        message = \"Wrong {}: '{}'\\n\\nAllowed values: {}\"\n",
    "        \n",
    "        if value not in allowed:\n",
    "            raise ValueError(message.format(\n",
    "                name,\n",
    "                value,\n",
    "                allowed\n",
    "            ))\n",
    "        \n",
    "        setattr(self, name, value)\n",
    "        \n",
    "    def check_value_type_and_set(self, name, value, allowed):\n",
    "        message = \"Wrong {} type: '{}'\\n\\nAllowed types: {}\"\n",
    "        \n",
    "        if not isinstance(value, allowed):\n",
    "            raise TypeError(message.format(\n",
    "                name,\n",
    "                value,\n",
    "                allowed\n",
    "            ))\n",
    "        \n",
    "        setattr(self, name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\n",
    "#     '../lab01/nyc-taxi-trip-duration/cleaned_train.csv'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(BasicModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        penalty='l2',\n",
    "        tol=1e-4,\n",
    "        C=1.0,\n",
    "        fit_intercept=True,\n",
    "        max_iter=100\n",
    "    ):\n",
    "        super().check_value_and_set(\n",
    "            'penalty',\n",
    "            penalty,\n",
    "            ['l1', 'l2', None]\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'tol',\n",
    "            tol,\n",
    "            (int, float)\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'C',\n",
    "            C,\n",
    "            (int, float)\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'fit_intercept',\n",
    "            fit_intercept,\n",
    "            bool\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'max_iter',\n",
    "            max_iter,\n",
    "            int\n",
    "        )\n",
    "    \n",
    "    def __get_l1_penalty(self):\n",
    "        def l1_penalty(w):\n",
    "            return 1/self.C * np.abs(w)\n",
    "        \n",
    "        def der_l1_penalty(w):\n",
    "            # ignoring zeros existence\n",
    "            return 1/self.C * ((w > 0) * 1 + (w <= 0) * -1)\n",
    "        \n",
    "        return l1_penalty, der_l1_penalty\n",
    "    \n",
    "    def __get_l2_penalty(self):\n",
    "        def l2_penalty(w):\n",
    "            return 1/self.C * np.multiply(w, w)\n",
    "        \n",
    "        def der_l2_penalty(w):\n",
    "            return 2/self.C * w\n",
    "        \n",
    "        return l2_penalty, der_l2_penalty\n",
    "    \n",
    "    def __get_None_penalty(self):\n",
    "        return None, None\n",
    "    \n",
    "    def fit(self, X, y, debug=False):\n",
    "        assert \\\n",
    "            len(X.shape) == 2, \\\n",
    "            \"X should be 2D vector\"\n",
    "        assert \\\n",
    "            y.shape == (X.shape[0], 1), \\\n",
    "            \"y should be 2D vector and should correspond to X\"\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        \n",
    "        if isinstance(y, (pd.DataFrame, pd.Series)):\n",
    "            y = y.to_numpy()\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((\n",
    "                X, \n",
    "                np.ones(\n",
    "                    (X.shape[0], 1)\n",
    "                )\n",
    "            ))\n",
    "        \n",
    "        args = [X, y]\n",
    "        \n",
    "        args.extend(\n",
    "            getattr(\n",
    "                self,\n",
    "                '_LogisticRegression__get_' + str(self.penalty) + '_penalty'\n",
    "            )()\n",
    "        )\n",
    "        \n",
    "        self.w = np.ones((X.shape[1], 1))\n",
    "#         self.w = np.random.rand(X.shape[1], 1)\n",
    "        \n",
    "        if debug:\n",
    "            return args\n",
    "        \n",
    "        result = sp.optimize.minimize(\n",
    "            self.__cost,\n",
    "            self.w,\n",
    "            args,\n",
    "            'L-BFGS-B',\n",
    "            self.__gradient,\n",
    "            tol=self.tol,\n",
    "            options={\n",
    "                'maxiter': self.max_iter\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        assert result.success, result.message\n",
    "        \n",
    "        self.w = result.x\n",
    "    \n",
    "    @staticmethod\n",
    "    def __predict(X, w):\n",
    "        def predict_real(x, w):\n",
    "            return x @ w\n",
    "\n",
    "        def sigmoid(z):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        \n",
    "        return sigmoid(predict_real(X, w))\n",
    "    \n",
    "    @staticmethod\n",
    "    def __cost(w, args):\n",
    "        X, y, penalty, _ = args\n",
    "        \n",
    "        predictions = LogisticRegression.__predict(X, w)\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        \n",
    "        cost0 = -(1 - y).T @ np.log(1 - predictions)\n",
    "        cost1 = -y.T @ np.log(predictions)\n",
    "        \n",
    "        penalty_part = penalty(w).sum() if penalty else 0\n",
    "        \n",
    "        final_cost = (cost0 + cost1).sum() / m + penalty_part\n",
    "        \n",
    "        return final_cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((\n",
    "                X, \n",
    "                np.ones(\n",
    "                    (X.shape[0], 1)\n",
    "                )\n",
    "            ))\n",
    "        return self.__predict(X, self.w)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __gradient(w, args):\n",
    "        X, y, _, der_penalty = args\n",
    "        w = w.reshape((-1, 1))\n",
    "        \n",
    "        predictions = LogisticRegression.__predict(X, w)\n",
    "        \n",
    "        penalty_part = der_penalty(w) if der_penalty else 0\n",
    "        \n",
    "        return X.T @ (predictions - y) + penalty_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import unittest\n",
    "\n",
    "def dummy_dataset():\n",
    "    X, y = make_classification(100, 20)\n",
    "    y = y.reshape((100, 1))\n",
    "    return X, y\n",
    "\n",
    "def prepare(debug=True, penalty=None):\n",
    "    X, y = dummy_dataset()\n",
    "\n",
    "    lr = LogisticRegression(penalty=penalty)\n",
    "\n",
    "    args = lr.fit(X, y, debug)\n",
    "    \n",
    "    return lr, X, y, args\n",
    "\n",
    "class TestLogisticRegression(unittest.TestCase):\n",
    "    def test_gradient(self):\n",
    "        lr, X, y, args = prepare()\n",
    "        \n",
    "        self.assertEqual(\n",
    "            lr._LogisticRegression__gradient(lr.w, args).shape,\n",
    "            (21, 1)\n",
    "        )\n",
    "        \n",
    "    def test_cost(self):\n",
    "        lr, X, y, args = prepare()\n",
    "        \n",
    "        self.assertEqual(\n",
    "            type(lr._LogisticRegression__cost(lr.w, args)),\n",
    "            np.float64\n",
    "        )\n",
    "    \n",
    "    def test_None(self):\n",
    "        lr, X, y, args = prepare(False)\n",
    "        score = roc_auc_score(y, lr.predict(X))\n",
    "        print(\"Score: {}\".format(score))\n",
    "        \n",
    "    def test_l1(self):\n",
    "        lr, X, y, args = prepare(False, 'l1')\n",
    "        score = roc_auc_score(y, lr.predict(X))\n",
    "        print(\"Score: {}\".format(score))\n",
    "    \n",
    "    def test_l2(self):\n",
    "        lr, X, y, args = prepare(False, 'l2')\n",
    "        score = roc_auc_score(y, lr.predict(X))\n",
    "        print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_None (__main__.TestLogisticRegression) ... ok\n",
      "test_cost (__main__.TestLogisticRegression) ... ok\n",
      "test_gradient (__main__.TestLogisticRegression) ... ok\n",
      "test_l1 (__main__.TestLogisticRegression) ... ok\n",
      "test_l2 (__main__.TestLogisticRegression) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.953525641025641\n",
      "Score: 0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAIL\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_l2 (__main__.TestLogisticRegression)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-36-30bff43b0f2f>\", line 47, in test_l2\n",
      "    lr, X, y, args = prepare(False, 'l2')\n",
      "  File \"<ipython-input-36-30bff43b0f2f>\", line 15, in prepare\n",
      "    args = lr.fit(X, y, debug)\n",
      "  File \"<ipython-input-35-d255132e90f9>\", line 111, in fit\n",
      "    assert result.success, result.message\n",
      "AssertionError: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.065s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f6ea75bb690>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['first-arg-is-ignored', '--verbose'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes this happens due to too large l2 normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_None (__main__.TestLogisticRegression) ... /home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:126: RuntimeWarning: divide by zero encountered in log\n",
      "/home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:126: RuntimeWarning: invalid value encountered in matmul\n",
      "/home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:114: RuntimeWarning: overflow encountered in exp\n",
      "/home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:127: RuntimeWarning: divide by zero encountered in log\n",
      "/home/a3nippo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:127: RuntimeWarning: invalid value encountered in matmul\n",
      "ok\n",
      "test_cost (__main__.TestLogisticRegression) ... ok\n",
      "test_gradient (__main__.TestLogisticRegression) ... ok\n",
      "test_l1 (__main__.TestLogisticRegression) ... ok\n",
      "test_l2 (__main__.TestLogisticRegression) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n",
      "Score: 0.9867947178871549\n",
      "Score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.042s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f505acaf910>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['first-arg-is-ignored', '--verbose'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitFunctions:\n",
    "    @staticmethod\n",
    "    def gini_impurity(obj, observation_indexes):        \n",
    "        node_y = obj.y[observation_indexes, :]\n",
    "\n",
    "        p_0 = (node_y == 0).sum() / node_y.shape[0]\n",
    "        p_1 = (node_y == 1).sum() / node_y.shape[0]\n",
    "\n",
    "        return 1 - p_0 * p_0 - p_1 * p_1\n",
    "    \n",
    "    @staticmethod\n",
    "    def gini_index(\n",
    "        obj,\n",
    "        _,\n",
    "        left_split_indexes, \n",
    "        right_split_indexes\n",
    "    ):\n",
    "        w_left = left_split_indexes.shape[0]\n",
    "        w_right = right_split_indexes.shape[0]\n",
    "\n",
    "        W = w_left + w_right\n",
    "\n",
    "        gini_index = w_left/W * SplitFunctions.gini_impurity(\n",
    "            obj,\n",
    "            left_split_indexes\n",
    "        )\n",
    "\n",
    "        gini_index += w_right/W * SplitFunctions.gini_impurity(\n",
    "            obj,\n",
    "            right_split_indexes\n",
    "        )\n",
    "\n",
    "        return gini_index\n",
    "    \n",
    "    @staticmethod\n",
    "    def entropy(obj, observation_indexes):\n",
    "        node_y = obj.y[observation_indexes, :]\n",
    "\n",
    "        p_0 = (node_y == 0).sum() / node_y.shape[0]\n",
    "        p_1 = (node_y == 1).sum() / node_y.shape[0]\n",
    "\n",
    "        result = 0\n",
    "\n",
    "        if p_0 != 0:\n",
    "            result -= p_0*np.log2(p_0)\n",
    "\n",
    "        if p_1 != 0:\n",
    "            result -= p_1*np.log2(p_1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(\n",
    "        obj,\n",
    "        observation_indexes,\n",
    "        left_split_indexes,\n",
    "        right_split_indexes\n",
    "    ):\n",
    "        entropy_before = SplitFunctions.entropy(\n",
    "            obj,\n",
    "            observation_indexes\n",
    "        )\n",
    "\n",
    "        entropy_after = SplitFunctions.entropy(\n",
    "            obj,\n",
    "            left_split_indexes\n",
    "        )\n",
    "\n",
    "        entropy_after += SplitFunctions.entropy(\n",
    "            obj,\n",
    "            right_split_indexes\n",
    "        )\n",
    "\n",
    "        return entropy_before - entropy_after\n",
    "\n",
    "    @staticmethod\n",
    "    def gain_ratio(\n",
    "        obj,\n",
    "        observation_indexes,\n",
    "        left_split_indexes,\n",
    "        right_split_indexes\n",
    "    ):\n",
    "        split_info = 0\n",
    "\n",
    "        w_left = left_split_indexes.shape[0]\n",
    "        if w_left != 0:\n",
    "            split_info += w_left * np.log2(w_left)\n",
    "\n",
    "        w_right = right_split_indexes.shape[0]\n",
    "        if w_right != 0:\n",
    "            split_info += w_right * np.log2(w_right)\n",
    "\n",
    "        information_gain = SplitFunctions.information_gain(\n",
    "            obj,\n",
    "            observation_indexes,\n",
    "            left_split_indexes,\n",
    "            right_split_indexes\n",
    "        )\n",
    "\n",
    "        return information_gain / split_info\n",
    "\n",
    "    @staticmethod\n",
    "    def __mse(\n",
    "        obj,\n",
    "        observation_indexes\n",
    "    ):\n",
    "        labels = obj.y[observation_indexes, :]\n",
    "\n",
    "        return ((labels - labels.mean())**2).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_sum(\n",
    "        obj,\n",
    "        _,\n",
    "        left_split_indexes,\n",
    "        right_split_indexes\n",
    "    ):\n",
    "        left_mse = SplitFunctions.__mse(obj, left_split_indexes)\n",
    "\n",
    "        right_mse = SplitFunctions.__mse(obj, right_split_indexes)\n",
    "\n",
    "        return left_mse + right_mse\n",
    "\n",
    "    @staticmethod\n",
    "    def __mae(\n",
    "        obj,\n",
    "        observation_indexes\n",
    "    ):\n",
    "        labels = obj.y[observation_indexes, :]\n",
    "\n",
    "        return (np.abs(labels - labels.mean())).sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def mae_sum(\n",
    "        obj,\n",
    "        _,\n",
    "        left_split_indexes,\n",
    "        right_split_indexes\n",
    "    ):\n",
    "        left_mae = SplitFunctions.__mae(obj, left_split_indexes)\n",
    "\n",
    "        right_mae = SplitFunctions.__mae(obj, right_split_indexes)\n",
    "\n",
    "        return left_mae + right_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import functools\n",
    "\n",
    "# CART DT (maybe)\n",
    "class DecisionTree(BasicModel):\n",
    "    class Node:\n",
    "        def __init__(\n",
    "            self,\n",
    "            feature,\n",
    "            value,\n",
    "            observation_indexes,\n",
    "            left=None,\n",
    "            right=None,\n",
    "            answer=None\n",
    "        ):\n",
    "            self.feature = feature\n",
    "            self.split_value = value\n",
    "            self.observation_indexes = observation_indexes\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.answer = answer\n",
    "            \n",
    "        def is_leaf(self):\n",
    "            return not self.left and not self.right\n",
    "    \n",
    "    criterion_name_to_calculator_method_name = {\n",
    "        'gini': SplitFunctions.gini_index,\n",
    "        'entropy': SplitFunctions.information_gain,\n",
    "        'gain_ratio': SplitFunctions.gain_ratio,\n",
    "        'mse': SplitFunctions.mse_sum,\n",
    "        'mae': SplitFunctions.mae_sum,\n",
    "    }\n",
    "    \n",
    "    criterion_name_to_cmp = {\n",
    "        'gini': lambda x, y: x-y,\n",
    "        'entropy': lambda x, y: y-x,\n",
    "        'gain_ratio': lambda x, y: y-x,\n",
    "        'mse': lambda x, y: y-x,\n",
    "        'mae': lambda x, y: y-x,\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        criterion='gini',\n",
    "        splitter='best',\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        max_features=None,\n",
    "        random_state=42,\n",
    "        debug=False\n",
    "    ):\n",
    "        super().check_value_and_set(\n",
    "            'criterion',\n",
    "            criterion,\n",
    "            ['gini', 'entropy', 'gain_ratio', 'mse', 'mae']\n",
    "        )\n",
    "        \n",
    "        self.__cmp_criterion_values = \\\n",
    "            self.criterion_name_to_cmp[criterion]\n",
    "        self.__calc_criterion_value = \\\n",
    "            self.criterion_name_to_calculator_method_name[criterion]\n",
    "        \n",
    "        if criterion in ['mse', 'mae']:\n",
    "            self.__task = 'regression'\n",
    "        else:\n",
    "            self.__task = 'classification'\n",
    "        \n",
    "        super().check_value_and_set(\n",
    "            'splitter',\n",
    "            splitter,\n",
    "            ['best', 'random']\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'max_depth',\n",
    "            max_depth,\n",
    "            (int, type(None))\n",
    "        )\n",
    "        if self.max_depth is None:\n",
    "            self.max_depth = float('inf')\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'min_samples_split',\n",
    "            min_samples_split,\n",
    "            (int, float)\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'max_features',\n",
    "            max_features,\n",
    "            (int, float, str, type(None))\n",
    "        )\n",
    "        \n",
    "        if type(max_features) == str:\n",
    "            super().check_value_and_set(\n",
    "                'max_features',\n",
    "                max_features,\n",
    "                ['auto', 'sqrt', 'log2']\n",
    "            )\n",
    "        if max_features == 'auto':\n",
    "            max_features = 'sqrt'\n",
    "            \n",
    "        super().check_value_type_and_set(\n",
    "            'random_state',\n",
    "            random_state,\n",
    "            (np.random.RandomState, int)\n",
    "        )\n",
    "        if type(random_state) == int:\n",
    "            self.random_state = np.random.RandomState(random_state)\n",
    "            \n",
    "        super().check_value_type_and_set(\n",
    "            'debug',\n",
    "            debug,\n",
    "            bool\n",
    "        )\n",
    "        \n",
    "        self.root = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        assert \\\n",
    "            len(X.shape) == 2, \\\n",
    "            \"X should be 2D vector\"\n",
    "        assert \\\n",
    "            y.shape == (X.shape[0], 1), \\\n",
    "            \"y should be 2D vector and should correspond to X\"\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        \n",
    "        if isinstance(y, (pd.DataFrame, pd.Series)):\n",
    "            y = y.to_numpy()\n",
    "        \n",
    "        self.X, self.y = X, y\n",
    "        \n",
    "        if self.max_features is None:\n",
    "            self.max_features = float('inf')\n",
    "        \n",
    "        elif type(self.max_features) == float:\n",
    "            self.max_features = np.floor(\n",
    "                X.shape[1] * self.max_features\n",
    "            )\n",
    "        \n",
    "        elif type(self.max_features) == str:\n",
    "            self.max_features = np.floor(\n",
    "                getattr(np, self.max_features)(X.shape[1])\n",
    "            )\n",
    "        \n",
    "        if not self.debug:\n",
    "            self.__construct_tree()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        assert self.root != None, \"Not fitted\"\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for el in X:\n",
    "            prediction = self.__predict_observation(el)\n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        return np.array(predictions).reshape((-1, 1))\n",
    "            \n",
    "    def __predict_observation(self, el):        \n",
    "        node = self.root\n",
    "        \n",
    "        while not node.is_leaf():\n",
    "            if el[node.feature] <= node.split_value:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        \n",
    "        return node.answer\n",
    "    \n",
    "    def __get_split_pairs_gen(\n",
    "        self,\n",
    "        feature,\n",
    "        observation_indexes\n",
    "    ):\n",
    "        node_x = self.X[observation_indexes, [feature]]\n",
    "        \n",
    "        observation_indexes, node_x = zip(\n",
    "            *sorted(\n",
    "                zip(\n",
    "                    observation_indexes,\n",
    "                    node_x.ravel()\n",
    "                ), \n",
    "                key=lambda x: x[1]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # to use numpy views\n",
    "        observation_indexes = np.array(observation_indexes)\n",
    "        \n",
    "        uniques = list(dict.fromkeys(node_x).keys())\n",
    "        \n",
    "        last_i = 0\n",
    "        id_unique = 0\n",
    "        \n",
    "        while id_unique < len(uniques) - 1:\n",
    "            for i in range(last_i, len(node_x)):\n",
    "                if node_x[i] > uniques[id_unique]:\n",
    "                    last_i = i\n",
    "                    break\n",
    "            \n",
    "            left_split_indexes = observation_indexes[:last_i]\n",
    "            right_split_indexes = observation_indexes[last_i:]\n",
    "            \n",
    "            yield left_split_indexes, right_split_indexes, uniques[id_unique]\n",
    "            \n",
    "            id_unique += 1\n",
    "    \n",
    "    BestFeatureSplit = namedtuple(\n",
    "        'BestFeatureSplit', \n",
    "        [\n",
    "            'criterion_value',\n",
    "            'split_value',\n",
    "            'left_split_indexes',\n",
    "            'right_split_indexes'\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # output can be (None, None)\n",
    "    def __find_best_split_by_feature(\n",
    "        self, \n",
    "        feature, \n",
    "        observation_indexes\n",
    "    ):\n",
    "        split_pairs_gen = self.__get_split_pairs_gen(\n",
    "            feature,\n",
    "            observation_indexes\n",
    "        )\n",
    "        \n",
    "        # find best split\n",
    "        best_split = DecisionTree.BestFeatureSplit(\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None\n",
    "        )\n",
    "        \n",
    "        for left_split_indexes, right_split_indexes, split_value in split_pairs_gen:\n",
    "            criterion_value = self.__calc_criterion_value(\n",
    "                self,\n",
    "                observation_indexes,\n",
    "                left_split_indexes,\n",
    "                right_split_indexes\n",
    "            )\n",
    "            \n",
    "            current_split = DecisionTree.BestFeatureSplit(\n",
    "                criterion_value,\n",
    "                split_value,\n",
    "                left_split_indexes,\n",
    "                right_split_indexes\n",
    "            )\n",
    "            \n",
    "            if best_split.criterion_value is None or \\\n",
    "               self.__cmp_criterion_values(\n",
    "                   criterion_value, \n",
    "                   best_split.criterion_value\n",
    "               ) < 0:\n",
    "                best_split = current_split\n",
    "        \n",
    "        return best_split\n",
    "\n",
    "    def __construct_tree_helper(self, observation_indexes, depth):\n",
    "        not_splitted_node = DecisionTree.Node(\n",
    "            None,\n",
    "            None,\n",
    "            observation_indexes,\n",
    "            answer=self.__get_answer(\n",
    "                observation_indexes\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if depth > self.max_depth or \\\n",
    "           observation_indexes.shape[0] < self.min_samples_split:\n",
    "            return not_splitted_node\n",
    "        \n",
    "        features = np.arange(self.X.shape[1])\n",
    "        if self.splitter == 'random':\n",
    "            self.random_state.shuffle(features)\n",
    "        \n",
    "        best_feature_split = None\n",
    "        best_feature = None\n",
    "    \n",
    "        feature_counter = 0 if self.max_features else float('inf')\n",
    "        \n",
    "        for feature in features:\n",
    "            if best_feature and \\\n",
    "               self.splitter == 'random' and \\\n",
    "               feature_counter > self.max_features:\n",
    "                break\n",
    "            \n",
    "            feature_split = self.__find_best_split_by_feature(\n",
    "                feature,\n",
    "                observation_indexes\n",
    "            )\n",
    "            \n",
    "            if feature_split.criterion_value is None:\n",
    "                continue\n",
    "            \n",
    "            if best_feature is None or \\\n",
    "               self.__cmp_criterion_values(\n",
    "                   feature_split.criterion_value,\n",
    "                   best_feature_split.criterion_value\n",
    "               ) < 0:\n",
    "                best_feature_split = feature_split\n",
    "                best_feature = feature\n",
    "            \n",
    "            feature_counter += 1\n",
    "        \n",
    "        if best_feature is None or \\\n",
    "           best_feature_split.criterion_value == 0:\n",
    "            return not_splitted_node\n",
    "        \n",
    "        node = DecisionTree.Node(\n",
    "            best_feature,\n",
    "            best_feature_split.split_value,\n",
    "            observation_indexes\n",
    "        )\n",
    "        \n",
    "        node.left = self.__construct_tree_helper(\n",
    "            best_feature_split.left_split_indexes,\n",
    "            depth + 1\n",
    "        )\n",
    "        \n",
    "        node.right = self.__construct_tree_helper(\n",
    "            best_feature_split.right_split_indexes,\n",
    "            depth + 1\n",
    "        )\n",
    "        \n",
    "        if node.is_leaf():\n",
    "            node.answer = self.__get_answer(\n",
    "                observation_indexes\n",
    "            )\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def __get_answer(self, observation_indexes):\n",
    "        labels = self.y[observation_indexes, :]\n",
    "        \n",
    "        if self.__task == 'classification':\n",
    "            ones = labels.sum()\n",
    "            threshold = labels.shape[0] / 2\n",
    "            \n",
    "            if ones > threshold:\n",
    "                return 1\n",
    "            elif ones < threshold:\n",
    "                return 0\n",
    "            else:\n",
    "                return self.random_state.randint(0, 1)\n",
    "        else:\n",
    "            return np.mean(labels)\n",
    "    \n",
    "    def __construct_tree(self):\n",
    "        observation_indexes = np.arange(self.y.shape[0])\n",
    "        \n",
    "        self.root = self.__construct_tree_helper(\n",
    "            observation_indexes, \n",
    "            1\n",
    "        )\n",
    "        \n",
    "        if self.root is None:\n",
    "            self.root = DecisionTree.Node(\n",
    "                None,\n",
    "                None,\n",
    "                observation_indexes\n",
    "            )\n",
    "            \n",
    "            self.answer = self.__get_answer(\n",
    "                observation_indexes\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "import unittest\n",
    "import time\n",
    "\n",
    "cl_X, cl_y = make_classification(100, 20)\n",
    "cl_y = cl_y.reshape((100, 1))\n",
    "\n",
    "regr_X, regr_y = make_regression(100, 20)\n",
    "regr_y = regr_y.reshape((100, 1))\n",
    "\n",
    "def time_fit_predict(\n",
    "    X, \n",
    "    y,\n",
    "    score_names=['ROC AUC'],\n",
    "    score_funcs=[roc_auc_score],\n",
    "    *args,\n",
    "    **kwargs\n",
    "):\n",
    "    start = time.time()\n",
    "    \n",
    "    dt = DecisionTree(*args, **kwargs)\n",
    "    dt.fit(X, y)\n",
    "    \n",
    "    for score_name, score_func in zip(score_names, score_funcs):\n",
    "        score = score_func(cl_y, dt.predict(cl_X))\n",
    "\n",
    "        print(\"{} criterion {} score: {}\".format(\n",
    "            kwargs['criterion'].capitalize(), \n",
    "            score_name,\n",
    "            score\n",
    "        ))\n",
    "    print(\"Time: {}\".format(time.time() - start))\n",
    "\n",
    "class TestDecisionTree(unittest.TestCase):\n",
    "    def test_gini_impurity(self):\n",
    "        dt = DecisionTree(debug=True)\n",
    "        \n",
    "        dt.fit(\n",
    "            np.array([[1, 2], [1, 2], [1, 2]]),\n",
    "            np.array([1, 0, 1]).reshape((-1, 1))\n",
    "        )\n",
    "        \n",
    "        self.assertEqual(\n",
    "            SplitFunctions.gini_impurity(\n",
    "                dt,\n",
    "                np.array([0, 2])\n",
    "            ),\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        self.assertEqual(\n",
    "            SplitFunctions.gini_impurity(\n",
    "                dt,\n",
    "                np.array([0, 1])\n",
    "            ),\n",
    "            0.5\n",
    "        )\n",
    "    \n",
    "    def test_gini(self):\n",
    "        time_fit_predict(cl_X, cl_y, criterion='gini')\n",
    "        \n",
    "    def test_entropy(self):\n",
    "        time_fit_predict(cl_X, cl_y, criterion='entropy')\n",
    "    \n",
    "    def test_gain_ratio(self):\n",
    "        time_fit_predict(cl_X, cl_y, criterion='gain_ratio')\n",
    "        \n",
    "    def test_mse(self):\n",
    "        time_fit_predict(\n",
    "            regr_X, \n",
    "            regr_y,\n",
    "            ['MSE', 'MAE'],\n",
    "            [mean_squared_error, mean_absolute_error],\n",
    "            criterion='mse'\n",
    "        )\n",
    "    \n",
    "    def test_mae(self):\n",
    "        time_fit_predict(\n",
    "            regr_X, \n",
    "            regr_y,\n",
    "            ['MSE', 'MAE'],\n",
    "            [mean_squared_error, mean_absolute_error],\n",
    "            criterion='mae'\n",
    "        )\n",
    "    \n",
    "    def test_max_features_and_random_state(self):\n",
    "        print('\\nDifference in results means it works')\n",
    "        \n",
    "        for i in range(3):\n",
    "            time_fit_predict(\n",
    "                cl_X,\n",
    "                cl_y,\n",
    "                splitter='random',\n",
    "                max_features='sqrt',\n",
    "                criterion='gini',\n",
    "                random_state=i\n",
    "            )\n",
    "    \n",
    "    def test_max_depth(self):\n",
    "        max_depth = 5\n",
    "        \n",
    "        def check_depth(node, depth=1):\n",
    "            if not node.is_leaf():\n",
    "                return check_depth(node.left, depth+1) and \\\n",
    "                       check_depth(node.right, depth+1)\n",
    "            \n",
    "            if depth > max_depth:\n",
    "                result = False\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        dt = DecisionTree(max_depth=max_depth)\n",
    "        dt.fit(cl_X, cl_y)\n",
    "        \n",
    "        self.assertEqual(\n",
    "            check_depth(dt.root),\n",
    "            True\n",
    "        )\n",
    "    \n",
    "    def test_min_samples_split(self):\n",
    "        min_samples_split = 10\n",
    "        \n",
    "        def check_samples_split(node):\n",
    "            if node.is_leaf():\n",
    "                return True\n",
    "            \n",
    "            is_satisfied = \\\n",
    "                node.observation_indexes.shape[0] >= min_samples_split\n",
    "            \n",
    "            return is_satisfied and \\\n",
    "                   check_samples_split(node.left) and \\\n",
    "                   check_samples_split(node.right)\n",
    "        \n",
    "        dt = DecisionTree(min_samples_split=min_samples_split)\n",
    "        dt.fit(cl_X, cl_y)\n",
    "        \n",
    "        self.assertEqual(\n",
    "            check_samples_split(dt.root),\n",
    "            True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_entropy (__main__.TestDecisionTree) ... ok\n",
      "test_gain_ratio (__main__.TestDecisionTree) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy criterion ROC AUC score: 1.0\n",
      "Time: 1.8193128108978271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_gini (__main__.TestDecisionTree) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain_ratio criterion ROC AUC score: 1.0\n",
      "Time: 1.956484079360962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_gini_impurity (__main__.TestDecisionTree) ... ok\n",
      "test_mae (__main__.TestDecisionTree) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini criterion ROC AUC score: 0.95\n",
      "Time: 0.29741954803466797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_max_depth (__main__.TestDecisionTree) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mae criterion MSE score: 5069.4900761966055\n",
      "Mae criterion MAE score: 56.77250136042764\n",
      "Time: 0.5493738651275635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_max_features_and_random_state (__main__.TestDecisionTree) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Difference in results means it works\n",
      "Gini criterion ROC AUC score: 0.95\n",
      "Time: 0.10825133323669434\n",
      "Gini criterion ROC AUC score: 0.9299999999999999\n",
      "Time: 0.10529065132141113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_min_samples_split (__main__.TestDecisionTree) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini criterion ROC AUC score: 0.94\n",
      "Time: 0.1279444694519043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_mse (__main__.TestDecisionTree) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mse criterion MSE score: 3199.892861356935\n",
      "Mse criterion MAE score: 42.80670221302634\n",
      "Time: 1.31022047996521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 6.846s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fe373432dd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['first-arg-is-ignored', '--verbose'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entropy and gain_ratio are slower due to calculation of logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
