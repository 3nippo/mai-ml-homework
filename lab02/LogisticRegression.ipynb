{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "%run basic_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(BasicModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        penalty='l2',\n",
    "        tol=1e-4,\n",
    "        C=1.0,\n",
    "        fit_intercept=True,\n",
    "        max_iter=100\n",
    "    ):\n",
    "        super().check_value_and_set(\n",
    "            'penalty',\n",
    "            penalty,\n",
    "            ['l1', 'l2', None]\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'tol',\n",
    "            tol,\n",
    "            (int, float)\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'C',\n",
    "            C,\n",
    "            (int, float)\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'fit_intercept',\n",
    "            fit_intercept,\n",
    "            bool\n",
    "        )\n",
    "        \n",
    "        super().check_value_type_and_set(\n",
    "            'max_iter',\n",
    "            max_iter,\n",
    "            int\n",
    "        )\n",
    "        \n",
    "        self.w = None\n",
    "    \n",
    "    def __get_l1_penalty(self):\n",
    "        def l1_penalty(w):\n",
    "            return 1/self.C * np.abs(w)\n",
    "        \n",
    "        def der_l1_penalty(w):\n",
    "            # ignoring the fact that the limit\n",
    "            # of the derivative modulus at zero does not exist\n",
    "            return 1/self.C * ((w > 0) * 1 + (w <= 0) * -1)\n",
    "        \n",
    "        return l1_penalty, der_l1_penalty\n",
    "    \n",
    "    def __get_l2_penalty(self):\n",
    "        def l2_penalty(w):\n",
    "            return 1/self.C * np.multiply(w, w)\n",
    "        \n",
    "        def der_l2_penalty(w):\n",
    "            return 2/self.C * w\n",
    "        \n",
    "        return l2_penalty, der_l2_penalty\n",
    "    \n",
    "    def __get_None_penalty(self):\n",
    "        return None, None\n",
    "    \n",
    "    def fit(self, X, y, debug=False):\n",
    "        X = super().check_and_transform_X(X)\n",
    "        y = super().check_and_transform_y(X, y)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((\n",
    "                X, \n",
    "                np.ones(\n",
    "                    (X.shape[0], 1)\n",
    "                )\n",
    "            ))\n",
    "        \n",
    "        args = [X, y]\n",
    "        \n",
    "        args.extend(\n",
    "            getattr(\n",
    "                self,\n",
    "                '_LogisticRegression__get_' + str(self.penalty) + '_penalty'\n",
    "            )()\n",
    "        )\n",
    "        \n",
    "        self.w = np.ones((X.shape[1], 1))\n",
    "#         self.w = np.random.rand(X.shape[1], 1)\n",
    "        \n",
    "        if debug:\n",
    "            return args\n",
    "        \n",
    "        result = sp.optimize.minimize(\n",
    "            self.__cost,\n",
    "            self.w,\n",
    "            args,\n",
    "            'L-BFGS-B',\n",
    "            self.__gradient,\n",
    "            tol=self.tol,\n",
    "            options={\n",
    "                'maxiter': self.max_iter\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.success = result.success \n",
    "        self.message = result.message\n",
    "        \n",
    "        self.w = result.x\n",
    "    \n",
    "    @staticmethod\n",
    "    def __predict(X, w):\n",
    "        def predict_real(x, w):\n",
    "            return x @ w\n",
    "\n",
    "        def sigmoid(z):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        \n",
    "        return sigmoid(predict_real(X, w))\n",
    "    \n",
    "    @staticmethod\n",
    "    def __cost(w, args):\n",
    "        X, y, penalty, _ = args\n",
    "        \n",
    "        predictions = LogisticRegression.__predict(X, w)\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        \n",
    "        cost0 = -(1 - y).T @ np.log(1 - predictions)\n",
    "        cost1 = -y.T @ np.log(predictions)\n",
    "        \n",
    "        penalty_part = penalty(w).sum() if penalty else 0\n",
    "        \n",
    "        final_cost = (cost0 + cost1).sum() / m + penalty_part\n",
    "        \n",
    "        return final_cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "        assert np.all(self.w != None), \"Not fitted\"\n",
    "        \n",
    "        X = super().check_and_transform_X(X)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((\n",
    "                X, \n",
    "                np.ones(\n",
    "                    (X.shape[0], 1)\n",
    "                )\n",
    "            ))\n",
    "        return self.__predict(X, self.w)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __gradient(w, args):\n",
    "        X, y, _, der_penalty = args\n",
    "        w = w.reshape((-1, 1))\n",
    "        \n",
    "        predictions = LogisticRegression.__predict(X, w)\n",
    "        \n",
    "        penalty_part = der_penalty(w) if der_penalty else 0\n",
    "        \n",
    "        return X.T @ (predictions - y) + penalty_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import unittest\n",
    "\n",
    "def dummy_dataset():\n",
    "    X, y = make_classification(100, 20)\n",
    "    y = y.reshape((100, 1))\n",
    "    return X, y\n",
    "\n",
    "def prepare(debug=True, penalty=None):\n",
    "    X, y = dummy_dataset()\n",
    "\n",
    "    lr = LogisticRegression(penalty=penalty)\n",
    "\n",
    "    args = lr.fit(X, y, debug)\n",
    "    \n",
    "    return lr, X, y, args\n",
    "\n",
    "class TestLogisticRegression(unittest.TestCase):\n",
    "    def test_gradient(self):\n",
    "        lr, X, y, args = prepare()\n",
    "        \n",
    "        self.assertEqual(\n",
    "            lr._LogisticRegression__gradient(lr.w, args).shape,\n",
    "            (21, 1)\n",
    "        )\n",
    "        \n",
    "    def test_cost(self):\n",
    "        lr, X, y, args = prepare()\n",
    "        \n",
    "        self.assertEqual(\n",
    "            type(lr._LogisticRegression__cost(lr.w, args)),\n",
    "            np.float64\n",
    "        )\n",
    "    \n",
    "    def test_None(self):\n",
    "        lr, X, y, args = prepare(False)\n",
    "        score = roc_auc_score(y, lr.predict(X))\n",
    "        print(\"Score: {}\".format(score))\n",
    "        \n",
    "    def test_l1(self):\n",
    "        lr, X, y, args = prepare(False, 'l1')\n",
    "        score = roc_auc_score(y, lr.predict(X))\n",
    "        print(\"Score: {}\".format(score))\n",
    "    \n",
    "    def test_l2(self):\n",
    "        lr, X, y, args = prepare(False, 'l2')\n",
    "        score = roc_auc_score(y, lr.predict(X))\n",
    "        print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_None (__main__.TestLogisticRegression) ... ok\n",
      "test_cost (__main__.TestLogisticRegression) ... ok\n",
      "test_gradient (__main__.TestLogisticRegression) ... ok\n",
      "test_l1 (__main__.TestLogisticRegression) ... ok\n",
      "test_l2 (__main__.TestLogisticRegression) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9912000000000001\n",
      "Score: 0.9803921568627451\n",
      "Score: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.038s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f10531b7b50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['first-arg-is-ignored', '--verbose'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
